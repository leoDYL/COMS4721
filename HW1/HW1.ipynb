{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS W4721 Spring 2020 Homework 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood, Linear Regression, \n",
    "# Bias-Variance Tradeoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name (uni@columbia.edu)\n",
    "#### January 26, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment I collaborated with the following people. Blank entries in this table\n",
    "means that I have worked on the corresponding parts on my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Problem | Collaborators with their UNIs | Part |\n",
    "| --- | --- | --- |\n",
    "| Problem 2 | | |\n",
    "| Problem 3 | | |\n",
    "| Problem 4 | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font color=white> \n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: Please briefly describe your academic/career goals and your expectations about the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will review the principle of _maximum likelihood estimation_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a coin which falls its heads up with probability $0 < \\theta < 1$. \n",
    "Each throw is a Bernoulli random variable $x = \\begin{cases}1, \\mbox{if falls heads up}\\\\\n",
    "0, \\mbox{if falls tails up}\\end{cases}$.\n",
    "     \n",
    "For a Bernoulli random variable $x$: the probability mass function of\n",
    "$x$ is given by: \n",
    "\n",
    "$$\\Pr( x; \\theta) = \\theta^x ( 1 - \\theta)^{(1 - x)}$$\n",
    "\n",
    "Suppose we repeat the coin toss $N$ times\n",
    "to collect the data  $\\{ x^{(i)}  \\}_{i = 1}^N$. \n",
    "Write the log likelihood function $\\ln \\mathcal{L}(\\theta; \\{ x^{(i)}\\}_{i = 1}^N)$ and \n",
    "the maximum likelihood estimation of $\\theta$,  $\\hat{\\theta}_{\\mathsf{MLE}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose instead we are given a die with $K$ sides with each side falling its heads up with\n",
    "probability $0 < \\theta_k < 1$. While we can represent the result of a throw using\n",
    "a categorical variable $x \\in \\{1, \\cdots, K\\}$, we can use $\\mbox{1-of-K}$ encoding:\n",
    "$$x = k \\ \\ \\ \\Leftrightarrow \\ \\ \\ \\mathbf{x} = [0, \\cdots, \n",
    "\\underbrace{1}_{\\tiny \\begin{array}{c} \n",
    "\\mbox{k-th}\\\\\n",
    "\\mbox{position}\\\\\n",
    ":= x_k\n",
    "\\end{array}}, \\cdots, 0]$$\n",
    "\n",
    "Each throw is a categorical random variable\n",
    "$\\mathbf{x} \\sim \\mathrm{Categorical}(\\theta_1, \\cdots, \\theta_K)$ such that\n",
    "$\\Pr(x_k = 1; \\mathbf{\\theta}) = \\theta_k$ and $\\sum\\limits_{k = 1}^K \\theta_k = 1$.\n",
    "The probability mass function at $\\mathbf{x}$ is given by: $$\\Pr( \\mathbf{x}; \\theta_1, \\cdots, \\theta_K)\n",
    " = \\prod\\limits_{k = 1}^K \\theta_k ^{x_k}$$\n",
    " \n",
    "Suppose we throw the die $N$ times and obtain the data\n",
    "$\\{ \\mathbf{x}^{(i)} \\}_{i = 1}^N$.\n",
    "Write the log likelihood function $\\ln \\mathcal{L}(\n",
    "\\mathbf{\\theta}; \\{ \\mathbf{x}^{(i)}\\}_{i = 1}^N )$ and \n",
    "the maximum likelihood estimation of $\\theta$,  $\\hat{\\theta}_{\\mathsf{MLE}}$. \n",
    "\n",
    "\n",
    "{\\bf Hint}: Once you obtain the log likelihood function \n",
    "$\\mathcal{L}(\n",
    "\\mathbf{\\theta}; \\{ \\mathbf{x}^{(i)}\\}_{i = 1}^N )$, you will need to add the Lagrangian multiplier part that takes \n",
    "the probability sum constraint $\\sum\\limits_{k = 1}^K \\theta_k = 1$. For $\\lambda \\in \\mathbb{R}$,\n",
    "the Lagrangian is:\n",
    "$$\\ln \\mathcal{L}_{\\lambda}(\\mathbf{\\theta}; \\{ \\mathbf{x}^{(i)}\\}_{i = 1}^N) = \n",
    "\\ln \\mathcal{L}(\\mathbf{\\theta}; \\{ \\mathbf{x}^{(i)}\\}_{i = 1}^N) + \n",
    "\\lambda \\left (1 - \\sum\\limits_{k = 1}^K \\theta_k \\right )$$\n",
    "Take the partial derivative of $\\ln \\mathcal{L}(\n",
    "\\mathbf{\\theta}; \\{ \\mathbf{x}^{(i)}\\}_{i = 1}^N )$ with respect to each $\\theta_k$ and $\\lambda$, set them to zero,\n",
    "and solve for each variable of $\\theta_k$. Appendix E of Bishop's book may be helpful for\n",
    "this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class we derived a MLE estimator for the univariate Gaussian assumption. For\n",
    " $\\{x^{(i)} \\in \\mathbb{R} \\}_{i = 1}^N$ i.i.d, we chose $p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\n",
    " \\exp \\left ( \\frac{- (x - \\mu)^2}{2 \\sigma^2} \\right )$ and solved for $\\hat{\\mu}_{\\mathsf{MLE}}$\n",
    " and $\\hat{\\sigma}^2_{\\mathsf{MLE}}$. Repeat this exercise for the multivariate case:\n",
    " now assume  $\\{ \\mathbf{x}^{(i)} \\in \\mathbb{R}^d \\}_{i = 1}^N$ and\n",
    " choose $p( \\mathbf{x} | \\mathbf{\\mu}, \\mathbf{\\Sigma}) =\n",
    " \\frac{1}{ (2 \\pi)^{\\frac{d}{2}} \\sqrt{ \\mathsf{det}(\\Sigma) }}\n",
    " \\exp \\left ( \\frac{- ( \\mathbf{x} - \\mathbf{\\mu})^T\n",
    " \\mathbf{\\Sigma}^{-1} ( \\mathbf{x} - \\mathbf{\\mu})\n",
    " }{2} \\right )$. Write the log likelihood function \n",
    " $\\ln \\mathcal{L}( \\{ \\mu, \\Sigma \\}; \\{ x^{(i)}\\}_{i = 1}^N\n",
    " )$ and solve for $\\hat{\\mu}_{\\mathsf{MLE}}$ and $\\hat{\\Sigma}_{\\mathsf{MLE}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will use a numerical optimization routine to obtain maximum likelihood estimate\n",
    "of parameters.\n",
    "\n",
    "Suppose $\\{ x^{(i) }\\in \\mathbb{R} \\}_{i = 1}^N$ with $x^{(i)} \\sim p(x ; x_0, \\gamma)$\n",
    "defined as:\n",
    "$$p( x ; x_0, \\gamma )= \\frac{1}{\\pi\\exp( \\gamma) \\left [ \n",
    "1 + \\left( \\frac{x - x_0}{\\exp(\\gamma)} \\right)^2 \\right ]}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that $p( x ; x_0, \\gamma )$ is a probability density function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that the mean $\\mathbb{E}_{x \\sim p( x ; x_0, \\gamma )} \\left [x \\right ]$ is undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Write the log likelihood function $\\ln \\mathcal{L}(\\{x_0, \\gamma\\}; \\{x^{(i)}\\}_{i = 1}^N)$ and\n",
    "the expression for \n",
    "$\\frac{\\partial \\ln \\mathcal{L}( \\{x_0, \\gamma\\}; \\{x^{(i)}\\}_{i = 1}^N)}\n",
    "{\\partial x_0}$ and $\\frac{\\partial \n",
    "\\ln \\mathcal{L}(\\{x_0, \\gamma\\}; \\{x^{(i)}\\}_{i = 1}^N)}\n",
    "{\\partial \\gamma}$.\n",
    "Plot the log likelihood value as a 3D surface plot: x-axis should run over $x_0$ and\n",
    "y-axis should run over $\\gamma$. z-axis should correspond to the log likelihood value at\n",
    "the corresponding $(x_0, \\gamma)$ pair. Include the plot in your writeup.\n",
    " Do the stationary points (solutions to the maximum likelihood equations) have closed form solutions?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to obtain an estimate of $\\mathbf{\\theta} = \\{ x_0, \\gamma \\}$ using the dataset **problem3.csv**.\n",
    "If you are using Python, it is helpful to utilize $\\mathsf{scipy.optimize.minimize}$ \n",
    "function. Choose gradient descent optimizer or a quasi-Newton optimizer such as BFGS.\n",
    "List the optimizer that was chosen for this problem with the initial iterate.\n",
    "Tabulate the coordinates of the iterates of the optmization process and the final converged solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement ridge regression estimator using gradient descent.\n",
    "Although the ridge regression does have a closed form solution, gradient descent\n",
    "form lets you avoid explicit matrix inversion and scale to larger data. We will see\n",
    "this in our subsequent lectures.\n",
    "\n",
    "For this problem, we assume we are given the labeled data pair:\\\\\n",
    "$ \\{( \\mathbf{x}^{(i)} \\in \\mathbb{R}^d, y^{(i)} \\in \\mathbb{R}) \\}_{i= 1}^{N}$.\n",
    "The regularized objective function in this case is given by:\n",
    "$$\\min\\limits_{b\\in \\mathbb{R},\\mathbf{w} \\in \\mathbb{R}^d} L(\n",
    "b, \\mathbf{w}; \\{( \\mathbf{x}^{(i)}, y^{(i)}  \\}_{i= 1}^{N}\n",
    ") \n",
    ":=\n",
    "\\frac{1}{N} \\sum\\limits_{i = 1}^N (y^{(i)} - ( b + \\mathbf{w}^T \\cdot \n",
    "\\mathbf{x}^{(i)}))^2 + \\lambda \\cdot || \\mathbf{w}||_2^2\n",
    "$$\n",
    "\n",
    "The pseudocode for performing gradient descent is given by the following algorithm.\n",
    "The main structure consists of a loop which continues for a given number of epochs\n",
    "$T$. $\\eta$ is the learning rate that controls the amount you want to step into the \n",
    "direction of the negative gradient, and $\\lambda$ is the regularization parameter.\n",
    "\n",
    "\n",
    "**function** GDRidge($S_{\\mathsf{train}} = \\{(\\mathbf{x}^{(i)} , y^{(i)}) \\}_{i= 1}^{N}$,\n",
    "$T$,  $\\eta$, $\\lambda$)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Initialize the bias term $b \\leftarrow 0$ and the slope $\\mathbf{w} \\leftarrow \n",
    "\\mathbf{0}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**for** $t = 1, \\cdots, T$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$b_{\\mathsf{new}} \\leftarrow b - \\eta \\cdot \\frac{\\partial \n",
    "L }{\\partial b} \n",
    "$, &nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w}_{\\mathsf{new}} \\leftarrow \\mathbf{w} - \\eta \\cdot \\frac{\\partial \n",
    "L  }{\\partial \\mathbf{w}}\n",
    "$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$b \\leftarrow b_{\\mathsf{new}}$, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\mathbf{w} \\leftarrow \\mathbf{w}_{\\mathsf{new}}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**end for**\n",
    "\n",
    "**end function**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the expression for the gradient update for $b$ and $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function $\\mathsf{GDRidge}$ described above. Please include \n",
    "your source code in the writeup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Boston housing data (\\url{https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html}) and scale the data appropriately: standardize the feature\n",
    "matrix and [0, 1] scale the y values. Choose $T = 100$ and $\\eta = 0.01$.\n",
    "\n",
    " For $\\lambda = 0.1$, \n",
    "provide a x-y\n",
    "plot with the epoch number as x-axis and plot the following quantities on the y-axis:\n",
    "\n",
    "- The value of regularized objective $L$ at the start of each epoch.\n",
    "- The 2-norm of the weight vector $\\mathbf{w}$ at the start of each epoch.\n",
    "\n",
    "Here is an example plot:\n",
    "\n",
    "![convergence](img/convergence.png)\n",
    "\n",
    "This will require you to modify the function written in Part (b) to compute the required\n",
    "quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next plot will examine how the coefficients for each of the features change as the\n",
    "regularization parameter is varied. \n",
    "\n",
    "Provide a coefficient path plot for the Boston housing data for $\\lambda = 10, 1, 0.1, 0.01, 0.001$.\n",
    "$x$-axis is for the regularization value and $y$-axis\n",
    "for the coefficient of the final converged iterate of your gradient descent algorithm. \n",
    "Make sure to scale the data appropriately. Choose $T = 100$ and $\\eta = 0.01$.\n",
    "\n",
    "Here is an example plot for a dataset with 7 features: there is a connected path for each\n",
    "of the 7 features as the regularization parameter is varied.\n",
    "\n",
    "![coef path](img/coef_path.png)\n",
    "\n",
    "What do you notice about the behavior of the coefficients\n",
    "as the regularization is varied? Include the coefficient path plot and your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDSH",
   "language": "python",
   "name": "pdsh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
